{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E-commerce Text Classification by Neural Language Processing (NLP)\n",
    "\n",
    "Task: Categorize the unseen products into 4 categories namely \"Electronics\", \"Household\", \"Books\", and \"Clothing & Accessories\".\n",
    "\n",
    "This task means we need to build a prediction machine learning based on a Neural Language Processing (NLP) model for product categorization using product names and descriptions on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-25 18:44:10.572041: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-25 18:44:10.584035: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-25 18:44:10.587579: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-25 18:44:10.597274: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-25 18:44:11.247816: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# 1. Import packages\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          category                                               text\n",
      "0        Household  Paper Plane Design Framed Wall Hanging Motivat...\n",
      "1        Household  SAF 'Floral' Framed Painting (Wood, 30 inch x ...\n",
      "2        Household  SAF 'UV Textured Modern Art Print Framed' Pain...\n",
      "3        Household  SAF Flower Print Framed Painting (Synthetic, 1...\n",
      "4        Household  Incredible Gifts India Wooden Happy Birthday U...\n",
      "...            ...                                                ...\n",
      "50420  Electronics  Strontium MicroSD Class 10 8GB Memory Card (Bl...\n",
      "50421  Electronics  CrossBeats Wave Waterproof Bluetooth Wireless ...\n",
      "50422  Electronics  Karbonn Titanium Wind W4 (White) Karbonn Titan...\n",
      "50423  Electronics  Samsung Guru FM Plus (SM-B110E/D, Black) Colou...\n",
      "50424  Electronics                   Micromax Canvas Win W121 (White)\n",
      "\n",
      "[50425 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 2. Data Loading\n",
    "\n",
    "CSV_PATH = \"Dataset/ecommerceDataset.csv\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, names = ['category', 'text'])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Household' 'Books' 'Clothing & Accessories' 'Electronics']\n"
     ]
    }
   ],
   "source": [
    "# Check the distinct category\n",
    "\n",
    "distinct_categories = df['category'].unique()\n",
    "print(distinct_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n",
      "Household                 19313\n",
      "Books                     11820\n",
      "Electronics               10621\n",
      "Clothing & Accessories     8671\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the values of each categories\n",
    "\n",
    "print(df['category'].value_counts())\n",
    "\n",
    "# All values are seem imbalanced.\n",
    "# So, class_weight is considered to be put in model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50425, 2)\n"
     ]
    }
   ],
   "source": [
    "# 3. Data Inspection\n",
    "\n",
    "# a. Shape of the data\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50425 entries, 0 to 50424\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   category  50425 non-null  object\n",
      " 1   text      50424 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 788.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# b. Data Info\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50425</td>\n",
       "      <td>50424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4</td>\n",
       "      <td>27802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Household</td>\n",
       "      <td>Think &amp; Grow Rich About the Author NAPOLEON HI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>19313</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         category                                               text\n",
       "count       50425                                              50424\n",
       "unique          4                                              27802\n",
       "top     Household  Think & Grow Rich About the Author NAPOLEON HI...\n",
       "freq        19313                                                 30"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c. Data Description\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category    0\n",
      "text        1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# d. Check the missing values\n",
    "\n",
    "print(df.isna().sum())\n",
    "\n",
    "# The text has one missing value,\n",
    "# Thus, we need to delete a row with the missing values.\n",
    "# Why?\n",
    "# Because there is no name or description of a product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category    0\n",
      "text        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Delecting the row\n",
    "\n",
    "updated_df = df.dropna(axis=0)\n",
    "\n",
    "df = updated_df\n",
    "\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22622\n"
     ]
    }
   ],
   "source": [
    "# d. Check duplication\n",
    "\n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Data Preprocessing\n",
    "\n",
    "# a. Seperate the feature and the target\n",
    "\n",
    "import sklearn.preprocessing\n",
    "\n",
    "feature = df['text'].values\n",
    "target = df['category'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b. Perform target encoding to the category column\n",
    "\n",
    "target_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "target_encoded = target_encoder.fit_transform(target)\n",
    "target_encoded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Books' 'Clothing & Accessories' 'Electronics' 'Household']\n"
     ]
    }
   ],
   "source": [
    "# Inverse transform the target enconding to check what numbers inside target encoding represents:\n",
    "\n",
    "sample_categories = target_encoder.inverse_transform([0,1,2,3]) # There are 4 categories\n",
    "print(sample_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Perform train-val-test split\n",
    "import sklearn.model_selection\n",
    "\n",
    "seed = 42\n",
    "X_train,X_split,y_train,y_split = sklearn.model_selection.train_test_split(feature,target_encoded,train_size=0.7,random_state=seed)\n",
    "X_val,X_test,y_val,y_test = sklearn.model_selection.train_test_split(X_split,y_split,train_size=0.5,random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1735123454.125597   11097 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1735123454.146845   11097 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1735123454.148099   11097 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1735123454.150156   11097 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1735123454.151343   11097 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1735123454.152482   11097 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1735123454.247109   11097 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1735123454.248386   11097 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1735123454.249553   11097 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-25 18:44:14.250661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2160 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# 6. Neural Language Processing (NLP) development\n",
    "\n",
    "# a. Tokenization\n",
    "# Tokenization will convert individual text (word) into integers,\n",
    "# the converted integers are called tokens.\n",
    "\n",
    "tokenizer = keras.layers.TextVectorization(\n",
    "    max_tokens=7500,\n",
    "    output_sequence_length = 500\n",
    "    )\n",
    "\n",
    "tokenizer.adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Acer 18.5 inch (46.99 cm) LED Monitor - EB192Q (Black) Specifications LED 18.5 '' ACER EB192Qb (B). Brand ACER Model EB192Qb Response Time 5 ms Max. Resolution 1366x768 @ 60Hz Contrast Ratio 100 million: 1 (ACM). Brightness 200 nits (cd / m2). Display 18.5 inch Color System 16.7 m POWER Supply (100V-240V): Internal Power Consumption (Off): 0.45W Power Consumption (Sleep): 14W Power Consumption (on): 18W. VGA Port 1 Port.\"\n",
      " 'SOUMIK ELECTRICALS 5-inch Subwoofer with Maximum 4 ohm(100 W) Thisb product is from the brand SOUMIK ELECTRICALS it presents a 5 Inch subwoofer with maximum 4 ohm and comes with 100W. Use in your home theatre.']\n"
     ]
    }
   ],
   "source": [
    "# Test how the tokenizer works\n",
    "\n",
    "sample_tokens = tokenizer(X_train[:2])\n",
    "print(X_train[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[3365 4947  173    1  147  152  622    1   55  595  152 4947 3365    1\n",
      "  1613  151 3365  535    1 1478   61  105 1551  739  649    1    1 1619\n",
      "  1972  115 1539   36    1 1495 1270    1 1373 3153  259 4947  173   65\n",
      "   146    1  805   59  781    1 1290   59 1111  253    1   59 1111  943\n",
      "     1   59 1111   15    1 1210  377   36  377    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   1    1    1 2745    9  472   91    1  614    1   43   10   17    2\n",
      "   151    1    1   13 1131    6  105  173 2745    9  472   91 6308    3\n",
      "    69    9    1   32    8   11   50 3043    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]], shape=(2, 500), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(sample_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Embedding\n",
    "# Embendding converts tokens into a long vector.\n",
    "# This long vector is a special representation that can help your machine learning model to understand the context of the words.\n",
    "\n",
    "embedding = keras.layers.Embedding(7500, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Model development\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "# a. NLP layers\n",
    "\n",
    "model.add(tokenizer)\n",
    "model.add(embedding)\n",
    "\n",
    "# b. Recurrent Neural Network (RNN)\n",
    "\n",
    "model.add(keras.layers.Bidirectional(keras.layers.LSTM(16,return_sequences=False)))\n",
    "model.add(keras.layers.Dropout(0.3))\n",
    "model.add(keras.layers.Dense(len(df['category'].unique()), activation='softmax'))\n",
    "\n",
    "# Note that the RNN layers is developed simple and added with dropout to prevent an overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Model compile\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total = 50424\n"
     ]
    }
   ],
   "source": [
    "# Before we train the model, we need to create the class weight in order to balance the data.\n",
    "\n",
    "books = df['category'].value_counts()['Books']\n",
    "clothing = df['category'].value_counts()['Clothing & Accessories']\n",
    "electronics = df['category'].value_counts()['Electronics']\n",
    "household = df['category'].value_counts()['Household']\n",
    "\n",
    "total = books + clothing + electronics + household\n",
    "print(\"Total =\", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13\n",
      "2.91\n",
      "2.37\n",
      "1.31\n"
     ]
    }
   ],
   "source": [
    "# Scaling by total / 2.0 helps keep the loss to a similar magnitude\n",
    "\n",
    "weight_for_0 = (1 / books) * (total / 2.0)\n",
    "weight_for_1 = (1 / clothing) * (total / 2.0)\n",
    "weight_for_2 = (1 / electronics) * (total / 2.0)\n",
    "weight_for_3 = (1 / household) * (total / 2.0)\n",
    "\n",
    "print(round(weight_for_0, 2)) # Books\n",
    "print(round(weight_for_1, 2)) # Clothing & Accesories\n",
    "print(round(weight_for_2, 2)) # Electronics\n",
    "print(round(weight_for_3, 2)) # Household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {0:weight_for_0, 1:weight_for_1, 2:weight_for_2, 3:weight_for_3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-25 18:44:18.020104: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 21ms/step - accuracy: 0.8231 - loss: 1.0673 - val_accuracy: 0.9681 - val_loss: 0.1252\n",
      "Epoch 2/20\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - accuracy: 0.9766 - loss: 0.1930 - val_accuracy: 0.9722 - val_loss: 0.1069\n",
      "Epoch 3/20\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 20ms/step - accuracy: 0.9851 - loss: 0.1190 - val_accuracy: 0.9769 - val_loss: 0.1025\n",
      "Epoch 4/20\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - accuracy: 0.9914 - loss: 0.0720 - val_accuracy: 0.9767 - val_loss: 0.1135\n",
      "Epoch 5/20\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 20ms/step - accuracy: 0.9947 - loss: 0.0440 - val_accuracy: 0.9736 - val_loss: 0.1230\n",
      "Epoch 6/20\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - accuracy: 0.9941 - loss: 0.0444 - val_accuracy: 0.9763 - val_loss: 0.1176\n",
      "Epoch 6: early stopping\n"
     ]
    }
   ],
   "source": [
    "# 9. Model Training\n",
    "\n",
    "logpath = \"tensorboard/nlp/\" + datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "\n",
    "tb = keras.callbacks.TensorBoard(logpath)\n",
    "es = keras.callbacks.EarlyStopping(patience=3,verbose=3)\n",
    "\n",
    "history = model.fit(X_train,y_train,validation_data=(X_val,y_val),epochs=20,batch_size=32,callbacks=[tb,es], class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9799 - loss: 0.0995\n",
      "[0.13389645516872406, 0.9754098653793335]\n"
     ]
    }
   ],
   "source": [
    "# Further evaluate with test data\n",
    "\n",
    "evaluation = model.evaluate(X_test,y_test)\n",
    "print(evaluation)\n",
    "\n",
    "# Result : [test_loss, test_accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97      1756\n",
      "           1       0.98      0.98      0.98      1312\n",
      "           2       0.96      0.97      0.97      1560\n",
      "           3       0.98      0.98      0.98      2936\n",
      "\n",
      "    accuracy                           0.98      7564\n",
      "   macro avg       0.97      0.98      0.97      7564\n",
      "weighted avg       0.98      0.98      0.98      7564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification reports - to evaluate accuracy and f1 score of the model\n",
    "\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "prediction = model.predict(X_test)\n",
    "prediction_index = np.argmax(prediction, axis = 1)\n",
    "\n",
    "model_report = classification_report(y_test, prediction_index)\n",
    "\n",
    "print(model_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Jack & Jones Men's Cotton  Sweater\n",
      "\n",
      "Clothing & Accessories\n"
     ]
    }
   ],
   "source": [
    "# 10. Test the model for predicting suitable category for each product descriptions.\n",
    "\n",
    "predictions = model.predict(X_test[:20])\n",
    "class_predictions = target_encoder.inverse_transform(np.argmax(predictions,axis=1))\n",
    "\n",
    "print(X_test[4])\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(class_predictions[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Save the necessary components\n",
    "\n",
    "import pickle\n",
    "\n",
    "# a. Tokenizer\n",
    "\n",
    "with open(\"tokenizer.json\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Target Encoder\n",
    "\n",
    "with open (\"target_encoder.json\", \"wb\") as f:\n",
    "    pickle.dump(target_encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# c. Model in .h5\n",
    "\n",
    "keras.models.save_model(model, \"saved_models/Classification.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
